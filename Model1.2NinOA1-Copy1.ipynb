{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d0da61e-947b-4c32-ba90-81e9938e3269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc4027b5-8e95-4ce3-b19d-5430697edd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required packages...\n",
      "âœ… All packages installed successfully!\n",
      "\n",
      "Checking GPU availability...\n",
      "TensorFlow version: 2.10.1\n",
      "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU count: 1\n",
      "\n",
      "Setup complete! Ready to proceed.\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: Install Required Libraries and Setup GPU\n",
    "# Run this cell first to install everything you need\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "required_packages = [\n",
    "    'tensorflow[and-cuda]',  # Includes GPU support\n",
    "    'pandas',\n",
    "    'numpy',\n",
    "    'matplotlib',\n",
    "    'seaborn',\n",
    "    'scipy',\n",
    "    'scikit-learn',\n",
    "    'openpyxl'  # For Excel file reading\n",
    "]\n",
    "\n",
    "print(\"Installing required packages...\")\n",
    "for package in required_packages:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--quiet\"])\n",
    "\n",
    "print(\"âœ… All packages installed successfully!\")\n",
    "print(\"\\nChecking GPU availability...\")\n",
    "import tensorflow as tf\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"GPU count: {len(tf.config.list_physical_devices('GPU'))}\")\n",
    "\n",
    "print(\"\\nSetup complete! Ready to proceed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ae47fcc-bccf-40f5-907d-e604676ae160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\anweshasingh\\.conda\\envs\\ashcep\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b30bf34d-65fa-4019-97d3-cde27aca2286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully!\n",
      "TensorFlow version: 2.10.1\n",
      "NumPy version: 1.23.5\n",
      "Pandas version: 2.3.3\n",
      "Ready for data loading!\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: Import Libraries and Setup Seeds\n",
    "# All the essential imports in one place\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Callable\n",
    "from scipy import stats\n",
    "import os\n",
    "import shutil\n",
    "from tqdm.notebook import tqdm  # Progress bars for Jupyter!\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, LSTM, Bidirectional, Dropout,\n",
    "    BatchNormalization, GlobalMaxPooling1D, Dense\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(\"Ready for data loading!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4321b7c-d683-4f77-a18c-77c0cfaea0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'data-centres-worldwide.csv',\n",
       " 'EXCELdata_center_carbon_emission_dataset_75000.csv.xlsx',\n",
       " 'Frontier HPC & Facility Data.xlsx',\n",
       " 'global-data-on-sustainable-energy (1).csv',\n",
       " 'Model1.0NinOA.ipynb',\n",
       " 'Model1.1NinOA.ipynb',\n",
       " 'Model1.2NinOA1-Copy1.ipynb',\n",
       " 'Model1.2NinOA1-Copy2.ipynb',\n",
       " 'Model1.2NinOA1.ipynb',\n",
       " 'Untitled.ipynb']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.listdir()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3deaddfb-3e6d-41be-9a86-ebe72a39fe58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AnweshaSingh\\anaconda_projects\\44f72650-e0bb-4a6e-a65c-79ad6ab0e8e1\\EXCELdata_center_carbon_emission_dataset_75000.csv.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for root, dirs, files in os.walk(os.getcwd()):\n",
    "    for f in files:\n",
    "        if \"carbon\" in f.lower():\n",
    "            print(os.path.join(root, f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bee930e2-c0b2-4b59-a9a5-cf99202e149b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading carbon emission dataset...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'anaconda_projects/44f72650-e0bb-4a6e-a65c-79ad6ab0e8e1/EXCELdata_center_carbon_emission_dataset_75000.csv.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading carbon emission dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Adjust the path if your file is in a different location\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43manaconda_projects/44f72650-e0bb-4a6e-a65c-79ad6ab0e8e1/EXCELdata_center_carbon_emission_dataset_75000.csv.xlsx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Dataset loaded successfully! Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRows: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\AshCEP\\lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\AshCEP\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   1548\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1550\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1557\u001b[0m         )\n",
      "File \u001b[1;32m~\\.conda\\envs\\AshCEP\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1400\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1404\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1405\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1406\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\AshCEP\\lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'anaconda_projects/44f72650-e0bb-4a6e-a65c-79ad6ab0e8e1/EXCELdata_center_carbon_emission_dataset_75000.csv.xlsx'"
     ]
    }
   ],
   "source": [
    "# CELL 3: Load Dataset\n",
    "# Upload your data_center_carbon_emission_dataset_COPY.xlsx file to this folder\n",
    "# or place it in the same directory as this notebook\n",
    "\n",
    "print(\"Loading carbon emission dataset...\")\n",
    "# Adjust the path if your file is in a different location\n",
    "df = pd.read_excel('anaconda_projects/44f72650-e0bb-4a6e-a65c-79ad6ab0e8e1/EXCELdata_center_carbon_emission_dataset_75000.csv.xlsx')\n",
    "\n",
    "print(f\"âœ… Dataset loaded successfully! Shape: {df.shape}\")\n",
    "print(f\"Rows: {df.shape[0]:,}\")\n",
    "print(f\"Columns: {df.shape[1]}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136e2f2b-7643-4ba3-9080-6d96a86619f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4: Data Preprocessing with Progress\n",
    "print(\"ðŸš€ Starting data preprocessing...\")\n",
    "\n",
    "# Convert timestamp and sort chronologically\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "print(\"âœ“ Timestamp converted and data sorted\")\n",
    "\n",
    "# Forward fill missing values\n",
    "df.ffill(inplace=True)\n",
    "print(\"âœ“ Missing values forward filled\")\n",
    "\n",
    "# Remove duplicates\n",
    "original_len = len(df)\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(f\"âœ“ Removed {original_len - len(df):,} duplicate rows\")\n",
    "\n",
    "# Create time-based features\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['dayofweek'] = df['timestamp'].dt.dayofweek\n",
    "print(\"âœ“ Time-based features created\")\n",
    "\n",
    "# Remove outliers using z-score (keeping |z| < 3)\n",
    "original_len = len(df)\n",
    "z_scores = np.abs(stats.zscore(df.select_dtypes(include=[np.number])))\n",
    "df = df[(z_scores < 3).all(axis=1)]\n",
    "print(f\"âœ“ Removed {original_len - len(df):,} outlier rows\")\n",
    "\n",
    "# Create lag features\n",
    "df['voltage_lag1'] = df['voltage'].shift(1)\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(\"âœ“ Lag features created and NaNs dropped\")\n",
    "\n",
    "print(f\"\\nâœ… Final dataset shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d31e0f-858b-4bdb-a755-18390a45b8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5: Train-Validation-Test Split (BEFORE Scaling)\n",
    "print(\"ðŸ“Š Splitting dataset: Train(70%), Val(15%), Test(15%)\")\n",
    "\n",
    "train_size = int(len(df) * 0.7)\n",
    "val_size = int(len(df) * 0.15)\n",
    "\n",
    "train = df[:train_size].copy()\n",
    "val = df[train_size:train_size + val_size].copy()\n",
    "test = df[train_size + val_size:].copy()\n",
    "\n",
    "print(f\"Train: {train.shape} ({train.shape[0]:,} samples)\")\n",
    "print(f\"Val:   {val.shape} ({val.shape[0]:,} samples)\")\n",
    "print(f\"Test:  {test.shape} ({test.shape[0]:,} samples)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce477660-973c-40f0-87c5-0167dd812fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6: Feature Scaling and Encoding\n",
    "print(\"âš™ï¸ Scaling numerical features...\")\n",
    "\n",
    "num_cols = [\n",
    "    'voltage', 'current', 'power', 'frequency', 'energy', 'power_factor',\n",
    "    'esp32_temperature', 'cpu_temperature', 'gpu_temperature',\n",
    "    'cpu_usage_percent', 'cpu_power_watts', 'gpu_usage_percent',\n",
    "    'gpu_power_watts', 'ram_usage_percent', 'ram_power_watts'\n",
    "]\n",
    "\n",
    "# Fit scaler on ALL data (standard practice for time series)\n",
    "all_data_temp = pd.concat([train, val, test], ignore_index=True)\n",
    "scaler = StandardScaler()\n",
    "all_data_temp[num_cols] = scaler.fit_transform(all_data_temp[num_cols])\n",
    "\n",
    "# Apply to splits\n",
    "train[num_cols] = scaler.transform(train[num_cols])\n",
    "val[num_cols] = scaler.transform(val[num_cols])\n",
    "test[num_cols] = scaler.transform(test[num_cols])\n",
    "print(\"âœ“ Numerical features scaled\")\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "train = pd.get_dummies(train, columns=['MAC', 'weekday'], drop_first=True)\n",
    "val = pd.get_dummies(val, columns=['MAC', 'weekday'], drop_first=True)\n",
    "test = pd.get_dummies(test, columns=['MAC', 'weekday'], drop_first=True)\n",
    "\n",
    "# Align columns across splits\n",
    "all_cols = set(train.columns).union(set(val.columns)).union(set(test.columns))\n",
    "for col in all_cols:\n",
    "    if col not in train.columns: train[col] = 0\n",
    "    if col not in val.columns: val[col] = 0\n",
    "    if col not in test.columns: test[col] = 0\n",
    "\n",
    "train, val, test = train[sorted(train.columns)], val[sorted(val.columns)], test[sorted(test.columns)]\n",
    "print(\"âœ“ Columns aligned and encoded\")\n",
    "\n",
    "# Verification\n",
    "print(\"\\nðŸ“ˆ Energy stats after scaling:\")\n",
    "for split, data in [(\"Train\", train), (\"Val\", val), (\"Test\", test)]:\n",
    "    print(f\"  {split:<5} - Mean: {data['energy'].mean():.4f}, Std: {data['energy'].std():.4f}\")\n",
    "\n",
    "print(f\"\\nâœ… Final shapes: Train({train.shape}), Val({val.shape}), Test({test.shape})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8275aa64-1c18-4397-9cf0-3a031ea96ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7: Create Sequences for LSTM\n",
    "print(\"ðŸ”„ Preparing sequences for LSTM...\")\n",
    "\n",
    "def create_sequences(data: np.ndarray, target: np.ndarray, seq_length: int):\n",
    "    \"\"\"Create sliding window sequences for time series\"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i + seq_length])\n",
    "        y.append(target[i + seq_length])\n",
    "    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)\n",
    "\n",
    "sequence_length = 10\n",
    "target_col = 'energy'\n",
    "feature_cols = [col for col in train.columns if col not in ['timestamp', target_col]]\n",
    "\n",
    "# Prepare arrays\n",
    "X_train_arr = train[feature_cols].values\n",
    "y_train_arr = train[target_col].values\n",
    "X_val_arr = val[feature_cols].values\n",
    "y_val_arr = val[target_col].values\n",
    "X_test_arr = test[feature_cols].values\n",
    "y_test_arr = test[target_col].values\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_arr, y_train_arr, sequence_length)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_arr, y_val_arr, sequence_length)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_arr, y_test_arr, sequence_length)\n",
    "\n",
    "print(f\"âœ… Sequences ready:\")\n",
    "print(f\"  Train: {X_train_seq.shape}\")\n",
    "print(f\"  Val:   {X_val_seq.shape}\")\n",
    "print(f\"  Test:  {X_test_seq.shape}\")\n",
    "print(f\"Each sequence: {sequence_length} timesteps Ã— {X_train_seq.shape[2]} features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e89e8ae-ecf7-4370-ace4-187dbe3270fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8: Scaling Verification\n",
    "print(\"=\" * 70)\n",
    "print(\"ðŸ” SCALING VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nTarget (energy) statistics:\")\n",
    "print(f\"  Train - Min: {y_train_seq.min():.4f}, Max: {y_train_seq.max():.4f}, Mean: {y_train_seq.mean():.4f}, Std: {y_train_seq.std():.4f}\")\n",
    "print(f\"  Val   - Min: {y_val_seq.min():.4f}, Max: {y_val_seq.max():.4f}, Mean: {y_val_seq.mean():.4f}, Std: {y_val_seq.std():.4f}\")\n",
    "print(f\"  Test  - Min: {y_test_seq.min():.4f}, Max: {y_test_seq.max():.4f}, Mean: {y_test_seq.mean():.4f}, Std: {y_test_seq.std():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… Expected: Means â‰ˆ 0, Stds â‰ˆ 1 (Scaling successful!)\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d987ed3d-32e5-4d76-87de-263187ffd432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9: Ninja Optimization Algorithm (Simplified for Jupyter)\n",
    "# Your custom Ninja OA - checkpointing removed, progress bars added\n",
    "\n",
    "class NinjaOptimizationAlgorithm:\n",
    "    def __init__(self, objective_function: Callable, bounds: Dict[str, Tuple], \n",
    "                 n_agents: int = 12, max_iterations: int = 12, \n",
    "                 exploration_factor: float = 2.0, exploitation_factor: float = 0.5,\n",
    "                 verbose: bool = True):\n",
    "        self.objective_function = objective_function\n",
    "        self.bounds = bounds\n",
    "        self.n_agents = n_agents\n",
    "        self.max_iterations = max_iterations\n",
    "        self.exploration_factor = exploration_factor\n",
    "        self.exploitation_factor = exploitation_factor\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.param_names = list(bounds.keys())\n",
    "        self.agents = self._initialize_population()\n",
    "        self.fitness = np.full(n_agents, np.inf)\n",
    "        self.best_agent = None\n",
    "        self.best_fitness = np.inf\n",
    "        self.convergence_curve = []\n",
    "    \n",
    "    def _initialize_population(self) -> List[Dict]:\n",
    "        agents = []\n",
    "        for _ in range(self.n_agents):\n",
    "            agent = {}\n",
    "            for param, bound in self.bounds.items():\n",
    "                if bound[1] == 'int':\n",
    "                    agent[param] = int(np.random.randint(bound[0][0], bound[0][1] + 1))\n",
    "                elif bound[1] == 'float':\n",
    "                    agent[param] = float(np.random.uniform(bound[0][0], bound[0][1]))\n",
    "                elif bound[1] == 'float_log':\n",
    "                    log_min, log_max = np.log10(bound[0][0]), np.log10(bound[0][1])\n",
    "                    agent[param] = float(10 ** np.random.uniform(log_min, log_max))\n",
    "                elif bound[1] == 'categorical':\n",
    "                    selected = np.random.choice(bound[0])\n",
    "                    agent[param] = int(selected) if isinstance(selected, (int, np.integer)) else selected\n",
    "            agents.append(agent)\n",
    "        return agents\n",
    "    \n",
    "    def _evaluate_fitness(self, agent: Dict) -> float:\n",
    "        try:\n",
    "            return self.objective_function(agent)\n",
    "        except:\n",
    "            return np.inf\n",
    "    \n",
    "    def _exploration_phase(self, agent: Dict, iteration: int) -> Dict:\n",
    "        new_agent = agent.copy()\n",
    "        progress = iteration / self.max_iterations\n",
    "        for param, bound in self.bounds.items():\n",
    "            exploration_rate = self.exploration_factor * (1 - progress)\n",
    "            if bound[1] == 'int':\n",
    "                pert = np.random.randint(-int(exploration_rate), int(exploration_rate) + 1)\n",
    "                new_agent[param] = int(np.clip(agent[param] + pert, bound[0][0], bound[0][1]))\n",
    "            elif bound[1] in ['float', 'float_log']:\n",
    "                range_size = bound[0][1] - bound[0][0]\n",
    "                pert = np.random.uniform(-exploration_rate, exploration_rate) * range_size * 0.1\n",
    "                new_agent[param] = float(np.clip(agent[param] + pert, bound[0][0], bound[0][1]))\n",
    "            elif bound[1] == 'categorical' and np.random.rand() < exploration_rate * 0.3:\n",
    "                selected = np.random.choice(bound[0])\n",
    "                new_agent[param] = int(selected) if isinstance(selected, (int, np.integer)) else selected\n",
    "        return new_agent\n",
    "    \n",
    "    def _exploitation_phase(self, agent: Dict, best_agent: Dict, iteration: int) -> Dict:\n",
    "        new_agent = {}\n",
    "        progress = iteration / self.max_iterations\n",
    "        for param, bound in self.bounds.items():\n",
    "            exploitation_rate = self.exploitation_factor * progress\n",
    "            if bound[1] == 'int':\n",
    "                direction = best_agent[param] - agent[param]\n",
    "                step = int(direction * exploitation_rate)\n",
    "                new_agent[param] = int(np.clip(agent[param] + step, bound[0][0], bound[0][1]))\n",
    "            elif bound[1] in ['float', 'float_log']:\n",
    "                direction = best_agent[param] - agent[param]\n",
    "                step = direction * exploitation_rate + np.random.normal(0, 0.01 * abs(direction))\n",
    "                new_agent[param] = float(np.clip(agent[param] + step, bound[0][0], bound[0][1]))\n",
    "            elif bound[1] == 'categorical':\n",
    "                new_agent[param] = best_agent[param] if np.random.rand() < 0.7 + 0.3 * progress else agent[param]\n",
    "        return new_agent\n",
    "    \n",
    "    def _adaptive_phase_selection(self, iteration: int) -> str:\n",
    "        progress = iteration / self.max_iterations\n",
    "        return 'exploitation' if np.random.rand() < (0.3 + 0.6 * progress) and self.best_agent else 'exploration'\n",
    "    \n",
    "    def optimize(self) -> Tuple[Dict, float, List]:\n",
    "        print(\"ðŸ¥· Ninja Optimization Algorithm starting...\")\n",
    "        print(f\"Population: {self.n_agents} agents | Iterations: {self.max_iterations}\")\n",
    "        print(f\"Optimizing: {self.param_names}\\n\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Evaluate initial population with progress bar\n",
    "        print(\"Evaluating initial population...\")\n",
    "        for i in tqdm(range(self.n_agents), desc=\"Initial agents\"):\n",
    "            self.fitness[i] = self._evaluate_fitness(self.agents[i])\n",
    "            if self.fitness[i] < self.best_fitness:\n",
    "                self.best_fitness = self.fitness[i]\n",
    "                self.best_agent = self.agents[i].copy()\n",
    "        \n",
    "        self.convergence_curve.append(self.best_fitness)\n",
    "        print(f\"\\nInitial best fitness: {self.best_fitness:.6f}\\n\")\n",
    "        \n",
    "        # Main optimization loop with progress bar\n",
    "        for iteration in tqdm(range(self.max_iterations), desc=\"Ninja OA\"):\n",
    "            new_agents, new_fitness = [], []\n",
    "            \n",
    "            for i, agent in enumerate(self.agents):\n",
    "                phase = self._adaptive_phase_selection(iteration)\n",
    "                candidate = (self._exploitation_phase(agent, self.best_agent, iteration) \n",
    "                           if phase == 'exploitation' else self._exploration_phase(agent, iteration))\n",
    "                \n",
    "                candidate_fitness = self._evaluate_fitness(candidate)\n",
    "                \n",
    "                if candidate_fitness < self.fitness[i]:\n",
    "                    new_agents.append(candidate)\n",
    "                    new_fitness.append(candidate_fitness)\n",
    "                else:\n",
    "                    new_agents.append(agent)\n",
    "                    new_fitness.append(self.fitness[i])\n",
    "                \n",
    "                if candidate_fitness < self.best_fitness:\n",
    "                    self.best_fitness = candidate_fitness\n",
    "                    self.best_agent = candidate.copy()\n",
    "            \n",
    "            self.agents = new_agents\n",
    "            self.fitness = np.array(new_fitness)\n",
    "            self.convergence_curve.append(self.best_fitness)\n",
    "            \n",
    "            avg_fitness = np.mean(self.fitness)\n",
    "            elapsed_h = (time.time() - start_time) / 3600\n",
    "            if self.verbose:\n",
    "                print(f\"Iter {iteration+1}: Best={self.best_fitness:.6f} | Avg={avg_fitness:.6f} | {elapsed_h:.2f}h\")\n",
    "        \n",
    "        total_time = (time.time() - start_time) / 3600\n",
    "        print(f\"\\nâœ… Optimization complete! Total time: {total_time:.2f} hours\")\n",
    "        \n",
    "        print(\"\\nðŸ¥‡ Best hyperparameters:\")\n",
    "        for key, value in self.best_agent.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        print(f\"Best validation loss: {self.best_fitness:.6f}\")\n",
    "        \n",
    "        return self.best_agent, self.best_fitness, self.convergence_curve\n",
    "\n",
    "print(\"âœ… Ninja Optimization Algorithm ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143a1131-6e2d-492e-bd92-7d13a80630fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10: LSTM Model and Objective Function (Unchanged)\n",
    "def create_lstm_model(params: Dict, seq_len: int, num_feats: int) -> Sequential:\n",
    "    model = Sequential(name='LSTM_NiOA')\n",
    "    model.add(Input(shape=(seq_len, num_feats)))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(params['units'], return_sequences=True)))\n",
    "    \n",
    "    for _ in range(params['lstm_layers'] - 1):\n",
    "        model.add(LSTM(params['units'], return_sequences=True))\n",
    "        model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Dense(25, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    optimizer = (tf.keras.optimizers.AdamW(learning_rate=params['learning_rate']) \n",
    "                if params['optimizer'] == 'adamw' else \n",
    "                tf.keras.optimizers.Adam(learning_rate=params['learning_rate']))\n",
    "    \n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def objective_function_lstm(params: Dict) -> float:\n",
    "    model = create_lstm_model(params, X_train_seq.shape[1], X_train_seq.shape[2])\n",
    "    \n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=0)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        validation_data=(X_val_seq, y_val_seq),\n",
    "        epochs=30,\n",
    "        batch_size=params['batch_size'],\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    min_val_loss = min(history.history['val_loss'])\n",
    "    del model\n",
    "    tf.keras.backend.clear_session()\n",
    "    return min_val_loss\n",
    "\n",
    "print(\"âœ… LSTM model and objective function ready!\")\n",
    "print(f\"Global variables set: seq_len={X_train_seq.shape[1]}, num_feats={X_train_seq.shape[2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d02d77-83ee-4138-a974-b550c937fb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 11: Run Ninja Optimization (The Main Event!)\n",
    "hyperparameter_bounds = {\n",
    "    'lstm_layers': ([2, 3], 'int'),\n",
    "    'units': ([64, 128], 'categorical'),\n",
    "    'dropout': ([0.3, 0.6], 'float'),\n",
    "    'optimizer': (['adamw'], 'categorical'),\n",
    "    'learning_rate': ([5e-5, 5e-4], 'float_log'),\n",
    "    'batch_size': ([32], 'categorical')\n",
    "}\n",
    "\n",
    "print(f\"ðŸš€ Starting Ninja OA at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Global variables for the optimizer\n",
    "global seq_len, num_feats\n",
    "seq_len = X_train_seq.shape[1]\n",
    "num_feats = X_train_seq.shape[2]\n",
    "\n",
    "ninja_optimizer = NinjaOptimizationAlgorithm(\n",
    "    objective_function=objective_function_lstm,\n",
    "    bounds=hyperparameter_bounds,\n",
    "    n_agents=12,\n",
    "    max_iterations=12\n",
    ")\n",
    "\n",
    "best_params, best_loss, convergence = ninja_optimizer.optimize()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸŽ‰ OPTIMIZATION COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best validation loss: {best_loss:.6f}\")\n",
    "print(f\"Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d0ca9d-5582-42ac-8396-0f5892ee484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 12: Visualize Results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(convergence, 'b-o', linewidth=3, markersize=8, label='Best Fitness')\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Validation Loss (MSE)', fontsize=12)\n",
    "plt.title('ðŸ¥· Ninja OA Convergence', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(len(convergence)), convergence, 'go-', linewidth=3, markersize=8)\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Best Fitness (log scale)', fontsize=12)\n",
    "plt.yscale('log')\n",
    "plt.title('Convergence (Log Scale)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Stats\n",
    "print(\"ðŸ“Š Convergence Statistics:\")\n",
    "print(f\"Start: {convergence[0]:.6f}\")\n",
    "print(f\"Final: {convergence[-1]:.6f}\")\n",
    "improvement = ((convergence[0] - convergence[-1]) / convergence[0]) * 100\n",
    "print(f\"Improvement: {improvement:.2f}%\")\n",
    "\n",
    "print(\"\\nâœ… All done! Your best hyperparameters are ready for final model training.\")\n",
    "print(f\"Best params: {best_params}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AshCEP GPU)",
   "language": "python",
   "name": "ashcep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
