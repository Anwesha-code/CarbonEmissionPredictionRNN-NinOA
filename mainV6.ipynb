{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "986aee44-5a4a-4806-be0c-c02a5fadc784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.1\n",
      "GPU available: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Imports\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "from config import *\n",
    "from data_preprocessing import load_and_prepare_data, split_by_timestamp\n",
    "from utils import (\n",
    "    set_seeds,\n",
    "    scale_numeric_features,\n",
    "    create_sequences,\n",
    "    to_python_types\n",
    ")\n",
    "from models import NinjaOptimizationAlgorithm, create_lstm_model\n",
    "from train import objective_function_lstm\n",
    "from evaluate import evaluate_model\n",
    "\n",
    "\n",
    "# GPU Safety\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "set_seeds(RANDOM_SEED)\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU available:\", len(gpus) > 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f283de3-cb98-4f94-b32d-75360eb3a4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running k=60 with SEQUENCE_LENGTH=120 ===\n",
      "Train shape: (2016429, 120, 17)\n",
      "Validation shape: (431997, 120, 17)\n",
      "Test shape: (431998, 120, 17)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Horizon to k = 60\n",
    "\n",
    "\n",
    "FORECAST_HORIZON = 60\n",
    "print(f\"\\n=== Running k={FORECAST_HORIZON} with SEQUENCE_LENGTH={SEQUENCE_LENGTH} ===\")\n",
    "\n",
    "df = load_and_prepare_data(DATA_PATH, k=FORECAST_HORIZON)\n",
    "\n",
    "# Fixed timestamp split\n",
    "t_train_end = df[\"server_timestamp\"].quantile(TRAIN_RATIO)\n",
    "t_val_end = df[\"server_timestamp\"].quantile(TRAIN_RATIO + VAL_RATIO)\n",
    "\n",
    "train_df, val_df, test_df = split_by_timestamp(df, t_train_end, t_val_end)\n",
    "\n",
    "TARGET = \"energy_delta_k\"\n",
    "\n",
    "feature_cols = [\n",
    "    c for c in train_df.columns\n",
    "    if c not in [\"server_timestamp\", \"energy\", TARGET]\n",
    "]\n",
    "\n",
    "X_train, y_train = train_df[feature_cols], train_df[TARGET]\n",
    "X_val, y_val = val_df[feature_cols], val_df[TARGET]\n",
    "X_test, y_test = test_df[feature_cols], test_df[TARGET]\n",
    "\n",
    "\n",
    "# Scale X (TRAIN ONLY)\n",
    "\n",
    "X_train, X_val, X_test, scaler = scale_numeric_features(\n",
    "    X_train.copy(), X_val.copy(), X_test.copy(), feature_cols\n",
    ")\n",
    "\n",
    "\n",
    "# Create sequences\n",
    "\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequences(\n",
    "    X_train.values, y_train.values, SEQUENCE_LENGTH\n",
    ")\n",
    "X_val_seq, y_val_seq = create_sequences(\n",
    "    X_val.values, y_val.values, SEQUENCE_LENGTH\n",
    ")\n",
    "X_test_seq, y_test_seq = create_sequences(\n",
    "    X_test.values, y_test.values, SEQUENCE_LENGTH\n",
    ")\n",
    "\n",
    "seq_len = X_train_seq.shape[1]\n",
    "num_feats = X_train_seq.shape[2]\n",
    "\n",
    "print(\"Train shape:\", X_train_seq.shape)\n",
    "print(\"Validation shape:\", X_val_seq.shape)\n",
    "print(\"Test shape:\", X_test_seq.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c336be35-8976-494f-bb3f-552e20b9dd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train range: 2021-08-05 12:44:50 → 2021-09-12 04:11:57\n",
      "Validation range: 2021-09-12 04:11:58 → 2021-11-26 02:11:39\n",
      "Test range: 2021-11-26 02:11:40 → 2021-12-04 08:17:32\n"
     ]
    }
   ],
   "source": [
    "print(\"Train range:\",\n",
    "      train_df[\"server_timestamp\"].min(),\n",
    "      \"→\",\n",
    "      train_df[\"server_timestamp\"].max())\n",
    "\n",
    "print(\"Validation range:\",\n",
    "      val_df[\"server_timestamp\"].min(),\n",
    "      \"→\",\n",
    "      val_df[\"server_timestamp\"].max())\n",
    "\n",
    "print(\"Test range:\",\n",
    "      test_df[\"server_timestamp\"].min(),\n",
    "      \"→\",\n",
    "      test_df[\"server_timestamp\"].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3572599-3b8b-4846-b7cf-7304890e5f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp split verified: strictly chronological.\n"
     ]
    }
   ],
   "source": [
    "assert train_df[\"server_timestamp\"].max() < val_df[\"server_timestamp\"].min()\n",
    "assert val_df[\"server_timestamp\"].max() < test_df[\"server_timestamp\"].min()\n",
    "print(\"Timestamp split verified: strictly chronological.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87ee9d3f-c453-4dde-84ca-274d2bcd2deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment directory: C:\\Users\\AnweshaSingh\\anaconda_projects\\IEEE_CEP_V6_1min\\results\\k60_seq120_20260212_152425\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Results folder\n",
    "\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "experiment_dir = os.path.join(\n",
    "    RESULTS_DIR,\n",
    "    f\"k60_seq{SEQUENCE_LENGTH}_{timestamp}\"\n",
    ")\n",
    "\n",
    "os.makedirs(experiment_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(experiment_dir, \"model\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(experiment_dir, \"predictions\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(experiment_dir, \"plots\"), exist_ok=True)\n",
    "\n",
    "print(\"Experiment directory:\", experiment_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbe5af28-1a43-4d1a-b63f-5bd0692c85c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splits and scaler saved.\n"
     ]
    }
   ],
   "source": [
    "# Save sequence tensors\n",
    "np.save(os.path.join(experiment_dir, \"X_train.npy\"), X_train_seq)\n",
    "np.save(os.path.join(experiment_dir, \"X_val.npy\"), X_val_seq)\n",
    "np.save(os.path.join(experiment_dir, \"X_test.npy\"), X_test_seq)\n",
    "\n",
    "np.save(os.path.join(experiment_dir, \"y_train.npy\"), y_train_seq)\n",
    "np.save(os.path.join(experiment_dir, \"y_val.npy\"), y_val_seq)\n",
    "np.save(os.path.join(experiment_dir, \"y_test.npy\"), y_test_seq)\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, os.path.join(experiment_dir, \"scaler.pkl\"))\n",
    "\n",
    "print(\"Data splits and scaler saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "937a0e6a-f08e-4ed7-8057-85dd867c6c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space: {'lstm_layers': ([2, 3], 'int'), 'units': ([64, 128], 'categorical'), 'dropout': ([0.3, 0.6], 'float'), 'optimizer': (['adamw'], 'categorical'), 'learning_rate': ([5e-05, 0.0005], 'float_log'), 'batch_size': ([32], 'categorical')}\n",
      "Train seq shape: (2016429, 120, 17)\n",
      "Validation seq shape: (431997, 120, 17)\n"
     ]
    }
   ],
   "source": [
    "print(\"Search space:\", HYPERPARAMETER_BOUNDS)\n",
    "print(\"Train seq shape:\", X_train_seq.shape)\n",
    "print(\"Validation seq shape:\", X_val_seq.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8fede54-7659-4498-b2a4-1e93cabff20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(X_train_seq) > 0, \"Empty training sequences!\"\n",
    "assert len(X_val_seq) > 0, \"Empty validation sequences!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be5636d6-a6a0-4999-bce9-8fbcf095d52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimisation subset sizes: 604928 129599\n",
      "Starting Ninja Optimization Algorithm...\n",
      "\n",
      "Time limit callback active\n",
      "Time limit callback active\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0241s vs `on_train_batch_end` time: 0.0242s). Check your callbacks.\n",
      "Time limit callback active\n",
      "Time limit callback active\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0236s vs `on_train_batch_end` time: 0.0253s). Check your callbacks.\n",
      "Time limit callback active\n",
      "Time limit callback active\n",
      "\n",
      "Iteration 1/6\n",
      "Time limit callback active\n",
      "Time limit callback active\n",
      "Time limit callback active\n",
      "Time limit callback active\n",
      "Time limit callback active\n",
      "Time limit callback active\n",
      "  Best validation loss so far: 1.3614169802167453e-05\n",
      "  Best hyperparameters so far:\n",
      "    lstm_layers: 2\n",
      "    units: 128\n",
      "    dropout: 0.6\n",
      "    optimizer: adamw\n",
      "    learning_rate: 0.0005\n",
      "    batch_size: 32\n",
      "\n",
      "Iteration 2/6\n",
      "Time limit callback active\n",
      "Time limit callback active\n",
      "Time limit callback active\n",
      "Time limit callback active\n",
      "Time limit callback active\n",
      "Time limit callback active\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0154s vs `on_train_batch_end` time: 0.0157s). Check your callbacks.\n",
      "  Best validation loss so far: 1.3553126336773857e-05\n",
      "  Best hyperparameters so far:\n",
      "    lstm_layers: 2\n",
      "    units: 64\n",
      "    dropout: 0.3\n",
      "    optimizer: adamw\n",
      "    learning_rate: 0.0005\n",
      "    batch_size: 32\n",
      "\n",
      "Iteration 3/6\n",
      "Time limit callback active\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0199s vs `on_train_batch_end` time: 0.0219s). Check your callbacks.\n",
      "Time limit callback active\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0217s vs `on_train_batch_end` time: 0.0288s). Check your callbacks.\n",
      "Time limit callback active\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0156s vs `on_train_batch_end` time: 0.0169s). Check your callbacks.\n",
      "Time limit callback active\n",
      "Time limit callback active\n",
      "Time limit callback active\n",
      "  Best validation loss so far: 1.3549309187510516e-05\n",
      "  Best hyperparameters so far:\n",
      "    lstm_layers: 2\n",
      "    units: 64\n",
      "    dropout: 0.3\n",
      "    optimizer: adamw\n",
      "    learning_rate: 0.0005\n",
      "    batch_size: 32\n",
      "\n",
      "Iteration 4/6\n",
      "Time limit callback active\n",
      "Time limit callback active\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0178s vs `on_train_batch_end` time: 0.0187s). Check your callbacks.\n",
      "Time limit callback active\n",
      "Time limit callback active\n",
      "Time limit callback active\n",
      "Time limit callback active\n",
      "  Best validation loss so far: 1.3549309187510516e-05\n",
      "  Best hyperparameters so far:\n",
      "    lstm_layers: 2\n",
      "    units: 64\n",
      "    dropout: 0.3\n",
      "    optimizer: adamw\n",
      "    learning_rate: 0.0005\n",
      "    batch_size: 32\n",
      "\n",
      "Iteration 5/6\n",
      "Time limit callback active\n",
      "Time limit callback active\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0108s vs `on_train_batch_end` time: 0.0213s). Check your callbacks.\n",
      "Time limit callback active\n",
      "Time limit callback active\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0245s vs `on_train_batch_end` time: 0.0339s). Check your callbacks.\n",
      "Time limit callback active\n",
      "Time limit callback active\n",
      "  Best validation loss so far: 1.3548929018725175e-05\n",
      "  Best hyperparameters so far:\n",
      "    lstm_layers: 3\n",
      "    units: 64\n",
      "    dropout: 0.3037738239542304\n",
      "    optimizer: adamw\n",
      "    learning_rate: 0.00047954629519078405\n",
      "    batch_size: 32\n",
      "\n",
      "Iteration 6/6\n",
      "Time limit callback active\n",
      "Time limit callback active\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0166s vs `on_train_batch_end` time: 0.0214s). Check your callbacks.\n",
      "Time limit callback active\n",
      "Time limit callback active\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0226s vs `on_train_batch_end` time: 0.0324s). Check your callbacks.\n",
      "Time limit callback active\n",
      "Time limit callback active\n",
      "  Best validation loss so far: 1.3547083653975278e-05\n",
      "  Best hyperparameters so far:\n",
      "    lstm_layers: 2\n",
      "    units: 64\n",
      "    dropout: 0.4182390933142627\n",
      "    optimizer: adamw\n",
      "    learning_rate: 0.00049147762299616\n",
      "    batch_size: 32\n",
      "\n",
      "Optimization completed in 5.48 hours\n",
      "Best params: {'lstm_layers': 2, 'units': 64, 'dropout': 0.4182390933142627, 'optimizer': 'adamw', 'learning_rate': 0.00049147762299616, 'batch_size': 32}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Subset for optimisation\n",
    "\n",
    "\n",
    "n_train_opt = int(len(X_train_seq) * OPT_SUBSET_RATIO)\n",
    "n_val_opt = int(len(X_val_seq) * OPT_SUBSET_RATIO)\n",
    "\n",
    "print(\"Optimisation subset sizes:\", n_train_opt, n_val_opt)\n",
    "\n",
    "ninja = NinjaOptimizationAlgorithm(\n",
    "    objective_function=lambda p: objective_function_lstm(\n",
    "        p,\n",
    "        X_train_seq[:n_train_opt],\n",
    "        y_train_seq[:n_train_opt],\n",
    "        X_val_seq[:n_val_opt],\n",
    "        y_val_seq[:n_val_opt]\n",
    "    ),\n",
    "    bounds=HYPERPARAMETER_BOUNDS,\n",
    "    n_agents=N_AGENTS,\n",
    "    max_iterations=MAX_ITERATIONS,\n",
    "    exploration_factor=EXPLORATION_FACTOR,\n",
    "    exploitation_factor=EXPLOITATION_FACTOR\n",
    ")\n",
    "\n",
    "best_params, best_loss, convergence = ninja.optimize()\n",
    "\n",
    "print(\"Best params:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc3a5317-d8d9-401b-96a6-b1bdcede88b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimisation artifacts saved.\n"
     ]
    }
   ],
   "source": [
    "# Save best params\n",
    "with open(os.path.join(experiment_dir, \"best_params.json\"), \"w\") as f:\n",
    "    json.dump(to_python_types(best_params), f, indent=4)\n",
    "\n",
    "# Save convergence\n",
    "np.save(os.path.join(experiment_dir, \"convergence.npy\"), convergence)\n",
    "\n",
    "print(\"Optimisation artifacts saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "802da8ab-882c-49e8-bb04-bf45db494d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23143"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "import gc\n",
    "\n",
    "K.clear_session()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22b40e5f-bca0-43bf-8592-72b4c340f3d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 12\u001b[0m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m create_lstm_model(best_params, seq_len, num_feats)\n\u001b[0;32m      5\u001b[0m early_stop \u001b[38;5;241m=\u001b[39m EarlyStopping(\n\u001b[0;32m      6\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     patience\u001b[38;5;241m=\u001b[39mFINAL_PATIENCE,\n\u001b[0;32m      8\u001b[0m     restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      9\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     10\u001b[0m )\n\u001b[1;32m---> 12\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_seq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train_seq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_seq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFINAL_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m###############reduced batch size\u001b[39;49;00m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     20\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Save model\u001b[39;00m\n\u001b[0;32m     23\u001b[0m model\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(experiment_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNiOA_DRNN_k60.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32m~\\.conda\\envs\\AshCEP\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\.conda\\envs\\AshCEP\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model = create_lstm_model(best_params, seq_len, num_feats)\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=FINAL_PATIENCE,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_seq,\n",
    "    y_train_seq,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    epochs=FINAL_EPOCHS,\n",
    "    batch_size= 16,                ###############reduced batch size\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save model\n",
    "model.save(os.path.join(experiment_dir, \"model\", \"NiOA_DRNN_k60.h5\"))\n",
    "\n",
    "print(\"Final model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9719e1f-1b10-44e8-af28-f169f8c642cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df, y_true, y_pred, explanation = evaluate_model(\n",
    "    model, X_test_seq, y_test_seq\n",
    ")\n",
    "\n",
    "print(results_df)\n",
    "print(explanation)\n",
    "\n",
    "# Save predictions\n",
    "np.save(os.path.join(experiment_dir, \"predictions\", \"y_test.npy\"), y_true)\n",
    "np.save(os.path.join(experiment_dir, \"predictions\", \"y_test_pred.npy\"), y_pred)\n",
    "\n",
    "# Save metrics\n",
    "with open(os.path.join(experiment_dir, \"metrics.json\"), \"w\") as f:\n",
    "    json.dump(\n",
    "        dict(zip(results_df[\"Metric\"], results_df[\"Value\"])),\n",
    "        f,\n",
    "        indent=4\n",
    "    )\n",
    "\n",
    "print(\"Evaluation artifacts saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08f2eb2-5320-4091-9d1d-a0c10d09a762",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    \"horizon\": FORECAST_HORIZON,\n",
    "    \"sequence_length\": SEQUENCE_LENGTH,\n",
    "    \"train_ratio\": TRAIN_RATIO,\n",
    "    \"val_ratio\": VAL_RATIO,\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"opt_subset_ratio\": OPT_SUBSET_RATIO,\n",
    "    \"opt_iterations\": MAX_ITERATIONS,\n",
    "    \"n_agents\": N_AGENTS,\n",
    "    \"final_epochs\": FINAL_EPOCHS,\n",
    "    \"final_patience\": FINAL_PATIENCE\n",
    "}\n",
    "\n",
    "with open(os.path.join(experiment_dir, \"training_config.json\"), \"w\") as f:\n",
    "    json.dump(metadata, f, indent=4)\n",
    "\n",
    "print(\"Training metadata saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19e6b2a2-e2bd-48c6-9c1b-94c8f347c7ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Predicted vs Actual\u001b[39;00m\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(\u001b[43my_true\u001b[49m, y_pred, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot([y_true\u001b[38;5;241m.\u001b[39mmin(), y_true\u001b[38;5;241m.\u001b[39mmax()],\n\u001b[0;32m      5\u001b[0m          [y_true\u001b[38;5;241m.\u001b[39mmin(), y_true\u001b[38;5;241m.\u001b[39mmax()],\n\u001b[0;32m      6\u001b[0m          linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted vs Actual (k=60)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_true' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predicted vs Actual\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_true, y_pred, alpha=0.5)\n",
    "plt.plot([y_true.min(), y_true.max()],\n",
    "         [y_true.min(), y_true.max()],\n",
    "         linestyle=\"--\")\n",
    "plt.title(\"Predicted vs Actual (k=60)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(experiment_dir, \"plots\", \"pred_vs_actual.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Residuals\n",
    "residuals = y_true - y_pred\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(residuals, bins=40, kde=True)\n",
    "plt.title(\"Residual Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(experiment_dir, \"plots\", \"residuals.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Training curve\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(history.history[\"loss\"], label=\"Train\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Val\")\n",
    "plt.legend()\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(experiment_dir, \"plots\", \"training_curve.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Convergence\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(convergence, marker=\"o\")\n",
    "plt.title(\"NinOA Convergence\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(experiment_dir, \"plots\", \"ninoa_convergence.png\"))\n",
    "plt.close()\n",
    "\n",
    "print(\"All plots saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c0eead-dd6d-4af7-86c0-f55eaff2d1c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5215cb-e616-47e9-895f-9acb3c5e64fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AshCEP GPU)",
   "language": "python",
   "name": "ashcep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
